# NILAY1556

## Exploring...
- **Diffusion Models Explained** (19-08-25)
  Explores Diffusion models, their SoTA status compared to VAEs and GANs, and the concept of latent space within these architectures.

- **Transformer Limitations & Solutions** (30-07-25)
  Analyzes Transformer limitations like quadratic memory complexity, diminishing returns with large context, and reasoning challenges. Discusses solutions such as sparse/low-rank attention (Flash Attention), biasing towards recent data (MEGA), selective attention (LaMDA, CEMA), and Chain-of-Thought for reasoning.

- **Deepmind AI & LLM Repositories** (29-07-25)
  Review of Google DeepMind's Opal project and exploration of key GitHub repositories including Graphrag Toolkit and Awesome LLM Apps for advanced large language model applications.
  [opal](https://opal.withgoogle.com/)
  [graphrag-toolkit](https://github.com/awslabs/graphrag-toolkit)

## Starred Repositories
- Starred [Growth-Kinetics/DiffMem](https://github.com/Growth-Kinetics/DiffMem) on 2025-08-21
- Starred [osmandkitay/aura](https://github.com/osmandkitay/aura) on 2025-08-07

## Forked Repositories
- Forked [osmandkitay/aura](https://github.com/NILAY1556/aura) on 2025-08-07

