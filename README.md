# NILAY1556

## Exploring...
- **Diffusion Models Explained** (19-08-25)
  Understanding Diffusion Models, the current state-of-the-art in AI, which have surpassed previous leading techniques like VAEs and GANs in generative tasks, focusing on their latent space representation.

- **Transformer Limitations & Solutions** (30-07-25)
  Analysis of Transformer limitations including quadratic memory complexity, diminishing returns with large context windows, and challenges in reasoning tasks. Solutions discussed include sparse attention, low-rank approximation, FlashAttention, biasing towards recent data, selective attention (MEGA, LaMDA, CEMA), and Chain-of-Thought (CoT) for reasoning.

- **AI Project & Tool Exploration** (29-07-25)
  Reviewed Google DeepMind's Opal project and explored GitHub repositories for LLM applications, including the Graphrag Toolkit.
  [opal](https://opal.withgoogle.com/)
  [graphrag-toolkit](https://github.com/awslabs/graphrag-toolkit)

## Starred Repositories
- Starred [Growth-Kinetics/DiffMem](https://github.com/Growth-Kinetics/DiffMem) on 2025-08-21
- Starred [osmandkitay/aura](https://github.com/osmandkitay/aura) on 2025-08-07

## Forked Repositories
- Forked [osmandkitay/aura](https://github.com/NILAY1556/aura) on 2025-08-07

