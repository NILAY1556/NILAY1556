# NILAY1556

## Exploring...
- **Understanding RoPE and Diffusion** (19-08-25)
  Learn about RoPE, a positional embedding technique, and diffusion models, the current state-of-the-art in generative AI, surpassing previous kings like VAEs and GANs in latent space generation.

- **Transformer Limitations and Solutions** (30-07-25)
  Explores Transformer limitations including quadratic memory complexity (addressed by sparse attention, low-rank approximation, and Flash Attention) and diminishing returns with large context windows (mitigated by recency biasing, weight updates like MEGA, and selective attention like LaMDA/CEMA). It also notes Transformers' strength in statistical correlation over symbolic reasoning, leading to approaches like Chain-of-Thought (CoT).

- **Explore Google DeepMind's Opal** (29-07-25)
  Review of Google DeepMind's Opal project, alongside exploration of key GitHub repositories for LLM applications, including Graphrag Toolkit.
  [opal](https://opal.withgoogle.com/)
  [graphrag-toolkit](https://github.com/awslabs/graphrag-toolkit)

## Starred Repositories
- Starred [Growth-Kinetics/DiffMem](https://github.com/Growth-Kinetics/DiffMem) on 2025-08-21
- Starred [osmandkitay/aura](https://github.com/osmandkitay/aura) on 2025-08-07
- Starred [sapientinc/HRM](https://github.com/sapientinc/HRM) on 2025-07-27

## Forked Repositories
- Forked [osmandkitay/aura](https://github.com/NILAY1556/aura) on 2025-08-07
- Forked [sapientinc/HRM](https://github.com/NILAY1556/HRM) on 2025-07-27

