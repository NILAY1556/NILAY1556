# NILAY1556

## Exploring...
- **Transformer Limitations and Solutions** (30-07-25)
  Analyzes transformer limitations including quadratic memory complexity, addressed by sparse or approximate attention (Flash Attention), diminishing returns with large context windows, tackled by recency biasing (MEGA) or selective attention (LaMDA, CEMA), and reasoning task challenges, where Chain-of-Thought (CoT) emerges as a solution.

- **Exploring AI & LLM Projects** (29-07-25)
  Reviewed Google DeepMind's Opal project and explored key GitHub repositories like graphrag-toolkit and awesome-llm-apps, focusing on advancements in AI and large language model applications.
  [opal](https://opal.withgoogle.com/)
  [graphrag-toolkit](https://github.com/awslabs/graphrag-toolkit)

- **DiffusionLM Architecture Explained** (28-07-25)
  Explored the DiffusionLM architecture and its recent hype, referencing key papers LaViDa and MMaDA for detailed insights into its foundational design.
  [LaViDa](https://arxiv.org/abs/2505.16839)
  [MMaDA](https://arxiv.org/abs/2505.15809)

## Starred Repositories
- Starred [osmandkitay/aura](https://github.com/osmandkitay/aura) on 2025-08-07
- Starred [sapientinc/HRM](https://github.com/sapientinc/HRM) on 2025-07-27

## Forked Repositories
- Forked [osmandkitay/aura](https://github.com/NILAY1556/aura) on 2025-08-07
- Forked [sapientinc/HRM](https://github.com/NILAY1556/HRM) on 2025-07-27

