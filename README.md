# NILAY1556

## Exploring...
- **Transformer Limitations & Solutions** (30-07-25)
  Explores Transformer limitations: quadratic memory complexity (addressable via sparse attention, low-rank approximation, FlashAttention), diminishing returns with large contexts (managed by recent data biasing like MEGA, selective attention like LaMDA/CEMA), and reasoning capabilities (statistical correlation vs. symbolic thinking, addressed by CoT).

- **Exploring AI Projects and Tools** (29-07-25)
  Reviewed Google DeepMind's Opal project and explored significant GitHub repositories, including the Graphrag Toolkit and Awesome LLM Apps, to understand current advancements in AI application development.
  [opal](https://opal.withgoogle.com/)
  [graphrag-toolkit](https://github.com/awslabs/graphrag-toolkit)

- **Diffusion Models Explained** (28-07-25)
  Explores DiffusionLM, a recent AI model, highlighting its architecture through research papers LaViDa and MMaDA, and its increased hype from Google DeepMind.
  [LaViDa](https://arxiv.org/abs/2505.16839)
  [MMaDA](https://arxiv.org/abs/2505.15809)

## Starred Repositories
- Starred [osmandkitay/aura](https://github.com/osmandkitay/aura) on 2025-08-07
- Starred [sapientinc/HRM](https://github.com/sapientinc/HRM) on 2025-07-27

## Forked Repositories
- Forked [osmandkitay/aura](https://github.com/NILAY1556/aura) on 2025-08-07
- Forked [sapientinc/HRM](https://github.com/NILAY1556/HRM) on 2025-07-27

